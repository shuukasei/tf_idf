//dfの計算 tfidf01.py
#coding:utf-8
import MeCab
import codecs
import string
import re

identify=string.maketrans('http','    ')
delEstr=string.punctuation+string.digits+'% '+string.hexdigits+string.ascii_letters
f=open('200401.txt.utf8','r')
data=f.readlines()
for text_id, text in enumerate(data):
        text=text.strip('\n')
        text=text.translate(identify,delEstr)

        tagger=MeCab.Tagger('')
        node= tagger.parseToNode(text)
        while node.next:
                node=node.next
                if node.feature.split(',')[0]=='名詞':
                        print "%s\t%s" %(node.surface,text_id+1)
              
//sort,uniqコマンドを使う
python tfidf01.py > dic1.txt
cat dic1.txt | uniq -c > dic2.txt
cut -b9- dic2.txt > dic3.txt
cut -f1 dic3.txt > dic4.txt
sort dic4.txt | uniq -c | sort -nrk 1 > dic5.txt

//tfの計算  tfidf02.py
coding:utf-8
import MeCab
import codecs
import string
import re
import math
import sys
#import tfidf_third

#docs=tfidf_third.docs
docs=28790

fv_df={}
df={}

f=open('dic5.txt','r')
data=f.readlines()
for stri in data:
        stri=stri.strip()
　　　　stri=stri.strip()
        s=stri.split(' ')
        k=s[1]
        fv_df[k]=float(s[0])
        df[k]=math.log(docs/fv_df[k])

#for key in fv_df.keys():
#       print 'word=%s, df=%s' %(key,fv_df[key])

identify=string.maketrans('http','    ')
delEstr=string.punctuation+string.digits+'% '+string.hexdigits+string.ascii_letters

words=[]
tf_word=[]
#data=codecs.open('sennto.txt','r','utf-8').readlines()
f=open('ten_200401.txt','r')
data=f.readlines()
for text_id, text in enumerate(data):
        text=text.strip('\n')
        text=text.translate(identify,delEstr)
        word=0
        tagger=MeCab.Tagger('')
        nodes={}
        node= tagger.parseToNode(text)
        while node.next:
                node=node.next
                word+=1
                if node.feature.split(',')[0]=='名詞':
                        nodes.setdefault(node.surface,0)
                        nodes[node.surface]+=1
        tf_word.append(nodes)
        words.append(word)
f.close()

for text_id, fv in enumerate(tf_word):
        if text_id < 10:
                n=0
                print '\nthis is the text: %d ' % (text_id+1)
                tf={}
                tf_idf={}
                for key in fv.keys():
                        tf[key]=float(fv[key])/words[text_id]
                      # tf[key]=format(tf[key],'.2f')
                        tf_idf[key]=(tf[key]*df[key],tf[key],df[key],fv[key],fv_df[key])
                tf_idf=sorted(tf_idf.items(),key=lambda x:x[1][0],reverse=True)
                for word,value in tf_idf:
                        n+=1
                        if n<11:
                                print '%s  \t\t tf_idf:%lf  \t tf:%lf  \t idf:%lf \t term_count: %d \t document_count: %d ' %(word,value[0],value[1],value[2],value[3],value[4])

def bunnka(text):
        h=open('onebunnka_200401.txt','w')
        bunn=text.split('。')
        for n in bunn:
                if n!='\n':
                        n=n.lstrip()
                        h.write(n+'。'+'\n')
        h.close()
def summary():
        sents={}
        f=open('onebunnka_200401.txt','r')
        data=f.readlines()
        for text_id, text in enumerate(data):
                text=text.strip('\n')
                text=text.translate(identify,delEstr)
                sumn=0.0
                tagger=MeCab.Tagger('')
                node= tagger.parseToNode(text)
                while node.next:
                        node=node.next
                        if node.feature.split(',')[0]=='名詞':
                                surface=node.surface
                                for surface in tfidf.keys():
                        #               print '%s, %lf'% (surface,tfidf[surface])
                                        sumn+=tfidf[surface]
        #       print '%s,%lf'% (text,sumn)
                sents[text]=sumn
        f.close()
        
        sents=sorted(sents.items(),key=lambda x:x[1],reverse=True)
        k=0
        for key in sents:
                k+=1
                if k<4:
                        print key[0]

file=open("ten_200401.txt",'r')
s=0
while 1:
        line=file.readline()
        if not line:
                break
        s+=1
        print '\n the summarization of %d'%(s)
        bunnka(line)
        summary()
file.close()
